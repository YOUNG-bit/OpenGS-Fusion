<div align=center>

# OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding

[Dianyi Yang](https://young-bit.github.io/Young-bit/),  Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, [Yufeng Yue](https://yfyue-bit.github.io/), Yi Yang*

<h3 align="center"> IROS 2025 </h3>

[Project](https://young-bit.github.io/opengs-fusion.github.io/) | [Video](https://www.youtube.com/watch?v=e-bHh_uMMxE&t)

![github (1)](./assets/resutls.gif)

</div>

This repository is intended to provide an engineering implementation of our paper, and we hope it will contribute to the community. If you have any questions, feel free to contact us. 

## Environments
Install requirements
```bash
conda create -n opengsfusion python==3.9
conda activate opengsfusion
conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia
conda install cmake
pip install -r requirements.txt
```
Also, PCL is needed for fast-gicp submodule.

Install submodules

```bash
conda activate opengsfusion
pip install submodules/diff-gaussian-rasterization
pip install submodules/simple-knn
pip install submodules/MobileSAM

export OPENGS_ENV=/path/to/your/anaconda3/envs/opengsfusion
pip install submodules/vdbfusion

cd submodules/fast_gicp
mkdir build
cd build
cmake ..
make
cd ..
python setup.py install --user
```


## Datasets

- Replica
  - Download
    ```bash
    bash download_replica.sh
    ```
  - Configure
  
    Please modify the directory structure to ours.

    The original structure
    ```bash
    Replica
        - room0
            - results (contain rgbd images)
                - frame000000.jpg
                - depth000000.jpg
                ...
            - traj.txt
        ...
    ```
    Our structure
    ```bash
    Replica
        - room0
            - images (contain rgb images)
                - frame000000.jpg
                ...
            - depth_images (contain depth images)
                - depth000000.jpg
                ...
            - traj.txt
        ...
    ```    

- Scannet
  - Download follow [scannet](http://www.scan-net.org/)

    Our structure
    ```bash
    data
        - scene0046_00
            - rbg (contain rgb images)
                - 0.png
                ...
            - depth (contain depth images)
                - 0.png
                ...
            - pose (contain poses)
                - 0.txt
                ...
            - traj.txt
        ...
    ```
    The traj.txt file here is generated by running `./datasets_process/convert_pose_2_traj.py`.

- Custom datasets:

    For custom datasets, you should format your data to match either the Replica or ScanNet dataset structures. Additionally, you'll need to create a camera configuration file specifying your camera's intrinsic parameters
    `config.txt`:
    ```yaml
    ## camera parameters
    W H fx fy cx cy depth_scale depth_trunc dataset_type
    640 480 577.590698 578.729797 318.905426 242.683609 1000.0 5.0 scannet
    ```

    You can put this config in `./config` directory.




## Run
- Replica
    ```bash
    bash ./bash/train_replica_with_sem.sh
    ```

- Scannet
    ```bash
    bash ./bash/train_scannet_with_sem.sh
    ```

The pipeline has two steps for each dataset:

* Feature Extraction: Runs mobilesamv2_clip.py to extract 2D SAM masks and CLIP features [TODO: add sam1.0]
* 3D Mapping: Runs opengs_fusion.py to build semantic GS maps [TODO: support online sam]

## Real-time demo 
### Using rerun.io viewer

Rerun viewer shows the means of trackable Gaussians, and rendered image from reconstructed 3dgs map.

The demo show here is supported by [GS_ICP_SLAM](https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM). 

![GIFMaker_me](https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM/assets/34827206/b4715071-2e4a-4d17-b7a2-612bbd32dbd0)

You just need to add --rerun_viewer to the command when running opengs_fusion.py. For example:

```bash
python opengs_fusion.py --dataset_path /path/to/dataset --config /path/to/config.txt --output_path /path/to/output --rerun_viewer
```



## Query after performing mapping

After completing the mapping process, you can visualize and interact with the semantic maps using the following commands:

### For Replica Dataset
```bash
python show_lang_embed.py \
    --dataset_path /path_to_replica/office0 \
    --config ./configs/Replica/caminfo.txt \
    --scene_npz /path_to_replica_output/office0/office0_default_each/gs_scene.npz \
    --dataset_type replica \
    --view_scale 2.0
```

### For scannet Dataset
```bash
python show_lang_embed.py \
    --dataset_path /path_to_scannet/scene0062_00 \
    --config ./configs/Scannet/scene0062_00.txt \
    --scene_npz /path_to_scannet_output/scene0062_00/default_with_sem/gs_scene.npz \
    --dataset_type scannet \
    --view_scale 3.0
```

Here, users can freely adjust the viewing angle in the interface. We also provide a text box for real-time querying and threshold adjustment. All tests were conducted on an Ubuntu system with a 2K resolution.

### Key Press Description

- **T**: Toggle between color and label display modes.
- **J**: Highlight selected object.
- **K**: Capture screenshot of current view.
- **O**: Print current view information.
- **M**: Switch between different camera views.
- **P**: Downsample the point cloud.
- **=**: Save current mask point cloud.
- **L**: Toggle voxel visualization.


## üôè Acknowledgments

This work builds upon the following outstanding open-source projects:

- [GS_ICP_SLAM](https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM) - For their foundational work on Gaussian Splatting with ICP-based SLAM
- [VDBFusion](https://github.com/PRBonn/vdbfusion) - For their efficient volumetric mapping framework

We're deeply grateful to the researchers behind these projects for sharing their work with the community.

## Cite
